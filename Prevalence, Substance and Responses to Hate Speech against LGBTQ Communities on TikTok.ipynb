{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13192c78-b83e-4e0d-bf19-b031e63b1f16",
   "metadata": {},
   "source": [
    "# Libraries and Mock Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b22a7a-efa1-4eb6-a7f9-f5f767e7f426",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "499a86bf-8d09-4aec-a457-58b3d7595dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jordi/.local/lib/python3.10/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "2025-04-24 16:34:54.410276: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-24 16:34:54.440532: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-24 16:34:55.291343: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[nltk_data] Downloading package wordnet to /home/jordi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# =======================\n",
    "# Standard & System Libraries\n",
    "# =======================\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "\n",
    "# =======================\n",
    "# Numerical and Data Libraries\n",
    "# =======================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =======================\n",
    "# NLP Libraries\n",
    "# =======================\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from textblob import TextBlob\n",
    "\n",
    "# =======================\n",
    "# Machine Learning & Deep Learning\n",
    "# =======================\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer,\n",
    "    TfidfVectorizer,\n",
    "    ENGLISH_STOP_WORDS\n",
    ")\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "\n",
    "# =======================\n",
    "# Transformers & Hugging Face\n",
    "# =======================\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    BertTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "# =======================\n",
    "# Topic Modeling & Clustering\n",
    "# =======================\n",
    "import hdbscan\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import (\n",
    "    KeyBERTInspired,\n",
    "    LlamaCPP,\n",
    "    MaximalMarginalRelevance,\n",
    "    PartOfSpeech\n",
    ")\n",
    "\n",
    "# =======================\n",
    "# Gensim (Topic Coherence)\n",
    "# =======================\n",
    "#from gensim.models import CoherenceModel\n",
    "#from gensim.corpora import Dictionary\n",
    "\n",
    "# =======================\n",
    "# Visualization\n",
    "# =======================\n",
    "import datamapplot\n",
    "import umap\n",
    "\n",
    "# =======================\n",
    "# Hugging Face Datasets\n",
    "# =======================\n",
    "from datasets import Dataset, concatenate_datasets, load_dataset\n",
    "\n",
    "# =======================\n",
    "# Other Libraries\n",
    "# =======================\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# =======================\n",
    "# Initial Setup\n",
    "# =======================\n",
    "nltk.download('wordnet')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# =======================\n",
    "# Environment Variables\n",
    "# =======================\n",
    "TF_ENABLE_ONEDNN_OPTS = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d74ab07-5434-4f40-a0fc-7cec628a80be",
   "metadata": {},
   "source": [
    "## Mock Data\n",
    "The following mock data has been generated with ChatGPT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0db9d8-870f-41b5-b5ad-08df4fd77c9c",
   "metadata": {},
   "source": [
    "Mock Data Labeled Datasets HS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca5e38cf-80a3-4ce9-b5f0-74818bd08573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mock_hate_labeled_data():\n",
    "    texts = [\n",
    "        \"This is a terrible comment.\",  # hate\n",
    "        \"You're the worst person ever.\",  # hate\n",
    "        \"I can't stand your opinions.\",  # hate\n",
    "        \"This is hateful and unacceptable.\",  # hate\n",
    "        \"I will report this offensive post.\",  # hate\n",
    "        \"Stop posting such hateful things.\",  # hate\n",
    "        \"This content is deeply offensive.\",  # hate\n",
    "        \"This is pure hate speech.\",  # hate\n",
    "        \"You should be banned for this.\",  # hate\n",
    "        \"This comment is abusive.\",  # hate\n",
    "        \"Great job on the project!\",  # non-hate\n",
    "        \"I agree with this viewpoint.\",  # non-hate\n",
    "        \"This is a very informative post.\",  # non-hate\n",
    "        \"Well done on explaining this topic.\",  # non-hate\n",
    "        \"I support this initiative fully.\",  # non-hate\n",
    "        \"This is a good contribution to the discussion.\",  # non-hate\n",
    "        \"Thank you for sharing this.\",  # non-hate\n",
    "        \"This is a positive and helpful comment.\",  # non-hate\n",
    "        \"Excellent work, keep it up!\",  # non-hate\n",
    "        \"I appreciate your insight on this matter.\"  # non-hate\n",
    "    ]\n",
    "    labels = [1] * 10 + [0] * 10  # First 10 are hate (1), last 10 are non-hate (0)\n",
    "    return pd.DataFrame({'text': texts, 'label': labels})\n",
    "\n",
    "mock_data_hate_labeled = generate_mock_hate_labeled_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818f3522-6cab-4e3c-ad9a-3a9250fc5562",
   "metadata": {},
   "source": [
    "Mock Data TikTok Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3964cd5f-6e6a-428d-bc8b-81bd0c2b5564",
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_data_comments = {\n",
    "    \"cid\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20],\n",
    "    \"text\": [\n",
    "        # Non-hate comments\n",
    "        \"I totally agree with this post!\",\n",
    "        \"Great job on explaining the topic.\",\n",
    "        \"This is such a well-written article, thanks for sharing!\",\n",
    "        \"Your insights are very helpful, I appreciate the effort.\",\n",
    "        \"This is an interesting perspective, and I learned something new.\",\n",
    "        \"I'm really impressed by the clarity of your explanation.\",\n",
    "        \"I couldn't have said it better myself. Well done!\",\n",
    "        \"This was a very informative read, thanks for posting it!\",\n",
    "        \"I really appreciate the thoughtfulness of your comments.\",\n",
    "        \"Such a positive contribution to the discussion!\",\n",
    "        \n",
    "        # Hateful comments\n",
    "        \"You're such an idiot for believing this garbage!\",\n",
    "        \"This is disgusting! People like you make me sick!\",\n",
    "        \"I can't stand how stupid you are, do us all a favor and stop talking.\",\n",
    "        \"Only morons would agree with this nonsense, you must be brain-dead!\",\n",
    "        \"You're a worthless piece of trash, no one cares about your opinion!\",\n",
    "        \"What a waste of time, you're nothing but a pathetic loser!\",\n",
    "        \"It's hard to believe anyone is as dumb as you. Just shut up already.\",\n",
    "        \"You're the reason why the world is so messed up. Go crawl under a rock!\",\n",
    "        \"If ignorance was a crime, you'd be serving life. This is just sad.\",\n",
    "        \"Do everyone a favor and disappear, your existence is pointless!\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "mock_comments_df = pd.DataFrame(mock_data_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9245bd53-7e5d-4fb5-8cb3-1ce39690079e",
   "metadata": {},
   "source": [
    "# Hate Speech Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f78b63-1dad-4769-87d8-af512feb8236",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b39a9d6-3d81-4fd1-a8ca-603ef57aa2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    if isinstance(text, list):\n",
    "        text = \" \".join(text)\n",
    "    elif not isinstance(text, str):\n",
    "        print(f\"Invalid text input: {text}\")\n",
    "        return \"\"\n",
    "    \n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove usernames, links, hashtags, and punctuation\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)  # Remove usernames\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)  # Remove links\n",
    "    text = re.sub(r\"#(\\w+)\", r\"\\1\", text)  # Remove hashtags\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    \n",
    "    # Correct spelling\n",
    "    # text = str(TextBlob(text).correct())\n",
    "    \n",
    "    # Remove any extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Preprocess comments dataset\n",
    "def preprocess_comments(data):\n",
    "    # Remove rows with missing text or duplicates\n",
    "    data.dropna(subset=['text'], inplace=True)\n",
    "    data = data.drop_duplicates(subset='text')\n",
    "    data['preprocessed_text'] = [preprocess_text(text) for text in tqdm(data['text'], desc=\"Processing Texts\")]\n",
    "    \n",
    "    return data\n",
    "\n",
    "def tokenize_function(data):\n",
    "    return tokenizer(data['text'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "def predict_hate(dataset, model, tokenizer):\n",
    "    # Create a DataLoader with batch size 16 and padding using the tokenizer\n",
    "    loader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=16, \n",
    "        collate_fn=DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    )\n",
    "    \n",
    "    # Prepare lists to store texts and predictions\n",
    "    texts, probabilities = [], []\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Iterate over batches\n",
    "        for batch in loader:\n",
    "            # Get model outputs\n",
    "            outputs = model(batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "            # Convert logits to probabilities using softmax\n",
    "            probs = torch.softmax(outputs.logits, dim=1)\n",
    "            \n",
    "            # Store text and probabilities\n",
    "            for i in range(batch['input_ids'].size(0)):\n",
    "                texts.append(tokenizer.decode(batch['input_ids'][i], skip_special_tokens=True))\n",
    "                probabilities.append(probs[i].tolist())\n",
    "\n",
    "    # Return results as a DataFrame\n",
    "    return pd.DataFrame({\n",
    "        'text': texts,\n",
    "        'non_hate_probability': [p[0] for p in probabilities],  # Probability for non-hate speech\n",
    "        'hate_probability': [p[1] for p in probabilities]   # Probability for hate speech\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to create and train a model with given hyperparameters\n",
    "def train_model_with_params(learning_rate, batch_size, num_epochs):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=500,\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=num_epochs,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=100,\n",
    "        save_total_limit=1,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_validation,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    \n",
    "    # Evaluate the model on the validation set\n",
    "    eval_results = trainer.evaluate()\n",
    "    eval_loss = eval_results[\"eval_loss\"]\n",
    "    \n",
    "    return eval_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d1a944-8fcb-47f4-aae2-45bdec77f3ac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Training Model with labeled data from literature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d58743c1-f015-4667-9f1a-2bfdf2fc45aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts: 100%|██████████| 20/20 [00:00<00:00, 50291.41it/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at GroNLP/hateBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "837e806c49f54d1dac6743138be134d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f03bcbce57d4b09807e33eb0189010e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d790b9b3abda4a1cbe282db47c5b8ac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mock_data_hate_labeled = preprocess_comments(mock_data_hate_labeled)\n",
    "mock_dataset_hate_labeled = Dataset.from_pandas(mock_data_hate_labeled)\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"GroNLP/hateBERT\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"GroNLP/hateBERT\", num_labels=2)\n",
    "\n",
    "\n",
    "# Split the dataset into train/test/validation with StratifiedShuffleSplit\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.4, random_state=42)\n",
    "train_indices, temp_indices = next(splitter.split(mock_dataset_hate_labeled['label'], mock_dataset_hate_labeled['label']))\n",
    "train_set = mock_dataset_hate_labeled.select(train_indices)\n",
    "temp_set = mock_dataset_hate_labeled.select(temp_indices)\n",
    "\n",
    "# Further split temp into validation and test sets\n",
    "temp_labels = temp_set['label']\n",
    "splitter_temp = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "validation_indices, test_indices = next(splitter_temp.split(temp_set['label'], temp_labels))\n",
    "validation_set = temp_set.select(validation_indices)\n",
    "test_set = temp_set.select(test_indices)\n",
    "\n",
    "\n",
    "# Tokenize the datasets\n",
    "tokenized_train = train_set.map(tokenize_function, batched=True)\n",
    "tokenized_validation = validation_set.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_set.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40bfef60-23a8-47e1-a5b6-bd08c7a6a219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with lr=1e-05, batch_size=8, num_epochs=3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Evaluation Loss: 0.6973150968551636\n",
      "Training with lr=1e-05, batch_size=8, num_epochs=5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:12, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Evaluation Loss: 0.6298425197601318\n",
      "Training with lr=1e-05, batch_size=16, num_epochs=3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Evaluation Loss: 0.5981082320213318\n",
      "Training with lr=1e-05, batch_size=16, num_epochs=5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:10, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Evaluation Loss: 0.541671633720398\n",
      "Training with lr=3e-05, batch_size=8, num_epochs=3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Evaluation Loss: 0.3540622591972351\n",
      "Training with lr=3e-05, batch_size=8, num_epochs=5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:14, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Evaluation Loss: 0.16752232611179352\n",
      "Training with lr=3e-05, batch_size=16, num_epochs=3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:06, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Evaluation Loss: 0.12503333389759064\n",
      "Training with lr=3e-05, batch_size=16, num_epochs=5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:11, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Evaluation Loss: 0.06535712629556656\n",
      "Training with lr=5e-05, batch_size=8, num_epochs=3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Evaluation Loss: 0.030310776084661484\n",
      "Training with lr=5e-05, batch_size=8, num_epochs=5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:14, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Evaluation Loss: 0.004212190397083759\n",
      "Training with lr=5e-05, batch_size=16, num_epochs=3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:06, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Evaluation Loss: 0.0015345459105446935\n",
      "Training with lr=5e-05, batch_size=16, num_epochs=5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:12, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Evaluation Loss: 0.0006386138265952468\n",
      "Best Hyperparameters: learning_rate=5e-05, batch_size=16, num_epochs=5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define hyperparameter grid as desired\n",
    "hyperparameter_grid = {\n",
    "    'learning_rate': [1e-5, 3e-5, 5e-5],\n",
    "    'per_device_train_batch_size': [8, 16],\n",
    "    'num_train_epochs': [3, 5]\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "best_params = None\n",
    "best_loss = np.inf\n",
    "\n",
    "for lr in hyperparameter_grid['learning_rate']:\n",
    "    for batch_size in hyperparameter_grid['per_device_train_batch_size']:\n",
    "        for num_epochs in hyperparameter_grid['num_train_epochs']:\n",
    "            print(f\"Training with lr={lr}, batch_size={batch_size}, num_epochs={num_epochs}\")\n",
    "            loss = train_model_with_params(lr, batch_size, num_epochs)\n",
    "            print(f\"Validation Evaluation Loss: {loss}\")\n",
    "            \n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                best_params = (lr, batch_size, num_epochs)\n",
    "\n",
    "print(f\"Best Hyperparameters: learning_rate={best_params[0]}, batch_size={best_params[1]}, num_epochs={best_params[2]}\")\n",
    "\n",
    "# Train final model with the best hyperparameters on the full training data\n",
    "best_learning_rate, best_batch_size, best_num_epochs = best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac455b13-b355-4cd0-bcaa-0afe94253916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:12, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Evaluation Loss: 0.0008615987026132643\n"
     ]
    }
   ],
   "source": [
    "final_training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    learning_rate=best_learning_rate,\n",
    "    per_device_train_batch_size=best_batch_size,\n",
    "    per_device_eval_batch_size=best_batch_size,\n",
    "    num_train_epochs=best_num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False\n",
    ")\n",
    "\n",
    "final_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=final_training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_validation,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "final_trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./SupervisedTrainingHateBERT\")\n",
    "tokenizer.save_pretrained(\"./SupervisedTrainingHateBERT\")\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_results = final_trainer.evaluate(eval_dataset=tokenized_test)\n",
    "test_loss = test_results[\"eval_loss\"]\n",
    "\n",
    "print(f\"Test Set Evaluation Loss: {test_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33726566-05d4-47a5-9644-87f2b99d6dbe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Predicting TikTok messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35cd5039-5677-45d0-8848-58df60403b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts: 100%|██████████| 20/20 [00:00<00:00, 50081.24it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model for hate speech detection\n",
    "tokenizer_hate = AutoTokenizer.from_pretrained(\"./SupervisedTrainingHateBERT\")\n",
    "model_hate = AutoModelForSequenceClassification.from_pretrained(\"./SupervisedTrainingHateBERT\")\n",
    "\n",
    "# Preprocess comments and get the preprocessed text\n",
    "comments_data = preprocess_comments(mock_comments_df)\n",
    "preprocessed_texts = comments_data['preprocessed_text'].to_list()\n",
    "\n",
    "# Tokenize preprocessed texts using the hate speech tokenizer\n",
    "encodings_hate = tokenizer_hate(preprocessed_texts, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "# Create dataset for hate speech predictions\n",
    "dataset_hate = Dataset.from_dict({\n",
    "    'input_ids': encodings_hate['input_ids'],\n",
    "    'attention_mask': encodings_hate['attention_mask']\n",
    "})\n",
    "\n",
    "# Make predictions\n",
    "predictions_df = predict_hate(dataset_hate, model_hate, tokenizer_hate)\n",
    "\n",
    "# You can then save the DataFrame to a CSV file if needed\n",
    "predictions_file = \"hateFirstTraining.csv\"\n",
    "predictions_df.to_csv(predictions_file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc20c73-348e-4931-b9bc-5a1b6ae4e397",
   "metadata": {},
   "source": [
    "## Obtain pseudolabels and retraining HS prediction model with labels and pseudo-labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d954ec4e-111d-40a0-b8ba-fdda08e5fe4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>youre a worthless piece of trash no one cares ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>such a positive contribution to the discussion</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  youre a worthless piece of trash no one cares ...      1\n",
       "1     such a positive contribution to the discussion      0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define pseudolabels\n",
    "hate_comments_predictions = pd.read_csv(\"hateFirstTraining.csv\")\n",
    "\n",
    "# Variables to control the pseudolabeling process\n",
    "hate_probability_threshold_pseudolabel = False\n",
    "non_hate_probability_threshold_pseudolabel = False\n",
    "\n",
    "# Hate pseudolabeling based on probability threshold or quantile\n",
    "if hate_probability_threshold_pseudolabel:\n",
    "    probability_threshold = 0.95\n",
    "    # Only keep rows with hate probability greater than the threshold\n",
    "    hate_pseudolabels = hate_comments_predictions[hate_comments_predictions[\"hate_probability\"] > probability_threshold]\n",
    "    hate_pseudolabels[\"label\"] = 1\n",
    "else:\n",
    "    quantile = 0.05\n",
    "    # Get the top 5% of hate comments based on the hate probability\n",
    "    top_5_percent_cutoff = hate_comments_predictions[\"hate_probability\"].quantile(1 - quantile)\n",
    "    hate_pseudolabels = hate_comments_predictions[hate_comments_predictions[\"hate_probability\"] >= top_5_percent_cutoff]\n",
    "    hate_pseudolabels[\"label\"] = 1\n",
    "\n",
    "# Non-hate pseudolabeling based on probability threshold or quantile\n",
    "if non_hate_probability_threshold_pseudolabel:\n",
    "    probability_threshold = 0.05\n",
    "    # Only keep rows with hate probability less than the threshold\n",
    "    non_hate_pseudolabels = hate_comments_predictions[hate_comments_predictions[\"hate_probability\"] < probability_threshold]\n",
    "    non_hate_pseudolabels[\"label\"] = 0\n",
    "else:\n",
    "    quantile = 0.05\n",
    "    # Get the bottom 5% of comments based on the hate probability\n",
    "    bottom_5_percent_cutoff = hate_comments_predictions[\"hate_probability\"].quantile(quantile)\n",
    "    non_hate_pseudolabels = hate_comments_predictions[hate_comments_predictions[\"hate_probability\"] <= bottom_5_percent_cutoff]\n",
    "    non_hate_pseudolabels[\"label\"] = 0\n",
    "\n",
    "# Combine the hate and non-hate pseudolabels\n",
    "hate_pseudolabels = pd.concat([hate_pseudolabels, non_hate_pseudolabels], ignore_index=True)\n",
    "\n",
    "# Display the final pseudolabels dataset\n",
    "hate_pseudolabels = hate_pseudolabels[[\"text\", \"label\"]]\n",
    "hate_pseudolabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "778b0af8-829d-4fe6-98cc-777643d6db20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at GroNLP/hateBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a619b48637b4f47ba3cadcc3d905aed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a381313489b46b39db9a75fa1b22ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "769161d639ad496989e74c4bb55b332b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"GroNLP/hateBERT\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"GroNLP/hateBERT\", num_labels=2)\n",
    "\n",
    "# Merge the DataFrames\n",
    "hate_dataset = pd.concat([mock_data_hate_labeled, hate_pseudolabels])\n",
    "\n",
    "hate_dataset = Dataset.from_pandas(hate_dataset)\n",
    "\n",
    "# Split the dataset into train/test/validation with StratifiedShuffleSplit\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_indices, temp_indices = next(splitter.split(hate_dataset['label'], hate_dataset['label']))\n",
    "train_set = hate_dataset.select(train_indices)\n",
    "temp_set = hate_dataset.select(temp_indices)\n",
    "\n",
    "# Further split temp into validation and test sets\n",
    "temp_labels = temp_set['label']\n",
    "splitter_temp = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "validation_indices, test_indices = next(splitter_temp.split(temp_set['label'], temp_labels))\n",
    "validation_set = temp_set.select(validation_indices)\n",
    "test_set = temp_set.select(test_indices)\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize the datasets\n",
    "tokenized_train = train_set.map(tokenize_function, batched=True)\n",
    "tokenized_validation = validation_set.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_set.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e205ce3-b827-4e29-852f-6afece7590f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with lr=1e-05, batch_size=8, num_epochs=5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:22, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Loss: 0.524088442325592\n",
      "Training with lr=3e-05, batch_size=8, num_epochs=5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:22, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Loss: 0.1373647302389145\n",
      "Training with lr=5e-05, batch_size=8, num_epochs=5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:22, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Loss: 0.017631273716688156\n",
      "Best Parameters: learning_rate=5e-05, batch_size=8, num_epochs=5\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameter grid\n",
    "hyperparameter_grid = {\n",
    "    'learning_rate': [1e-5, 3e-5, 5e-5],\n",
    "    'per_device_train_batch_size': [8],\n",
    "    'num_train_epochs': [5]\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "best_params = None\n",
    "best_loss = np.inf\n",
    "\n",
    "for lr in hyperparameter_grid['learning_rate']:\n",
    "    for batch_size in hyperparameter_grid['per_device_train_batch_size']:\n",
    "        for num_epochs in hyperparameter_grid['num_train_epochs']:\n",
    "            print(f\"Training with lr={lr}, batch_size={batch_size}, num_epochs={num_epochs}\")\n",
    "            loss = train_model_with_params(lr, batch_size, num_epochs)\n",
    "            print(f\"Evaluation Loss: {loss}\")\n",
    "            \n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                best_params = (lr, batch_size, num_epochs)\n",
    "\n",
    "print(f\"Best Parameters: learning_rate={best_params[0]}, batch_size={best_params[1]}, num_epochs={best_params[2]}\")\n",
    "\n",
    "# Use the best hyperparameters to train and save the final model\n",
    "best_lr, best_batch_size, best_num_epochs = best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b251e06-1a08-461d-a3af-cef189848063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:22, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset results: {'eval_loss': 0.0033087944611907005, 'eval_runtime': 0.2363, 'eval_samples_per_second': 12.695, 'eval_steps_per_second': 4.232, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./Semi_SupervisedTrainingHateBERT/tokenizer_config.json',\n",
       " './Semi_SupervisedTrainingHateBERT/special_tokens_map.json',\n",
       " './Semi_SupervisedTrainingHateBERT/vocab.txt',\n",
       " './Semi_SupervisedTrainingHateBERT/added_tokens.json',\n",
       " './Semi_SupervisedTrainingHateBERT/tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    learning_rate=best_lr,\n",
    "    per_device_train_batch_size=best_batch_size,\n",
    "    per_device_eval_batch_size=best_batch_size,\n",
    "    num_train_epochs=best_num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False\n",
    ")\n",
    "\n",
    "final_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=final_training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_validation,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "final_trainer.train()\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_results = final_trainer.evaluate(tokenized_test)\n",
    "print(\"Test dataset results:\", test_results)\n",
    "\n",
    "# Save the final model and tokenizer\n",
    "model.save_pretrained(\"./Semi_SupervisedTrainingHateBERT\")\n",
    "tokenizer.save_pretrained(\"./Semi_SupervisedTrainingHateBERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c32b95-0712-4b29-a392-161993c43853",
   "metadata": {},
   "source": [
    "## Predicting final HS probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07190636-3950-498a-a870-b5ab2d2ff1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts: 100%|██████████| 20/20 [00:00<00:00, 58991.62it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>non_hate_probability</th>\n",
       "      <th>hate_probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i totally agree with this post</td>\n",
       "      <td>0.998085</td>\n",
       "      <td>0.001915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>great job on explaining the topic</td>\n",
       "      <td>0.998515</td>\n",
       "      <td>0.001485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this is such a wellwritten article thanks for ...</td>\n",
       "      <td>0.998351</td>\n",
       "      <td>0.001649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>your insights are very helpful i appreciate th...</td>\n",
       "      <td>0.998318</td>\n",
       "      <td>0.001682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this is an interesting perspective and i learn...</td>\n",
       "      <td>0.997467</td>\n",
       "      <td>0.002533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>im really impressed by the clarity of your exp...</td>\n",
       "      <td>0.997870</td>\n",
       "      <td>0.002130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i couldnt have said it better myself well done</td>\n",
       "      <td>0.995290</td>\n",
       "      <td>0.004710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>this was a very informative read thanks for po...</td>\n",
       "      <td>0.998488</td>\n",
       "      <td>0.001512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i really appreciate the thoughtfulness of your...</td>\n",
       "      <td>0.998372</td>\n",
       "      <td>0.001628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>such a positive contribution to the discussion</td>\n",
       "      <td>0.998791</td>\n",
       "      <td>0.001209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>youre such an idiot for believing this garbage</td>\n",
       "      <td>0.007942</td>\n",
       "      <td>0.992058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>this is disgusting people like you make me sick</td>\n",
       "      <td>0.005415</td>\n",
       "      <td>0.994585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>i cant stand how stupid you are do us all a fa...</td>\n",
       "      <td>0.003708</td>\n",
       "      <td>0.996292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>only morons would agree with this nonsense you...</td>\n",
       "      <td>0.015346</td>\n",
       "      <td>0.984654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>youre a worthless piece of trash no one cares ...</td>\n",
       "      <td>0.004831</td>\n",
       "      <td>0.995169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>what a waste of time youre nothing but a pathe...</td>\n",
       "      <td>0.008088</td>\n",
       "      <td>0.991912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>its hard to believe anyone is as dumb as you j...</td>\n",
       "      <td>0.008026</td>\n",
       "      <td>0.991974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>youre the reason why the world is so messed up...</td>\n",
       "      <td>0.009599</td>\n",
       "      <td>0.990401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>if ignorance was a crime youd be serving life ...</td>\n",
       "      <td>0.076038</td>\n",
       "      <td>0.923962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>do everyone a favor and disappear your existen...</td>\n",
       "      <td>0.004847</td>\n",
       "      <td>0.995153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  non_hate_probability  \\\n",
       "0                      i totally agree with this post              0.998085   \n",
       "1                   great job on explaining the topic              0.998515   \n",
       "2   this is such a wellwritten article thanks for ...              0.998351   \n",
       "3   your insights are very helpful i appreciate th...              0.998318   \n",
       "4   this is an interesting perspective and i learn...              0.997467   \n",
       "5   im really impressed by the clarity of your exp...              0.997870   \n",
       "6      i couldnt have said it better myself well done              0.995290   \n",
       "7   this was a very informative read thanks for po...              0.998488   \n",
       "8   i really appreciate the thoughtfulness of your...              0.998372   \n",
       "9      such a positive contribution to the discussion              0.998791   \n",
       "10     youre such an idiot for believing this garbage              0.007942   \n",
       "11    this is disgusting people like you make me sick              0.005415   \n",
       "12  i cant stand how stupid you are do us all a fa...              0.003708   \n",
       "13  only morons would agree with this nonsense you...              0.015346   \n",
       "14  youre a worthless piece of trash no one cares ...              0.004831   \n",
       "15  what a waste of time youre nothing but a pathe...              0.008088   \n",
       "16  its hard to believe anyone is as dumb as you j...              0.008026   \n",
       "17  youre the reason why the world is so messed up...              0.009599   \n",
       "18  if ignorance was a crime youd be serving life ...              0.076038   \n",
       "19  do everyone a favor and disappear your existen...              0.004847   \n",
       "\n",
       "    hate_probability  \n",
       "0           0.001915  \n",
       "1           0.001485  \n",
       "2           0.001649  \n",
       "3           0.001682  \n",
       "4           0.002533  \n",
       "5           0.002130  \n",
       "6           0.004710  \n",
       "7           0.001512  \n",
       "8           0.001628  \n",
       "9           0.001209  \n",
       "10          0.992058  \n",
       "11          0.994585  \n",
       "12          0.996292  \n",
       "13          0.984654  \n",
       "14          0.995169  \n",
       "15          0.991912  \n",
       "16          0.991974  \n",
       "17          0.990401  \n",
       "18          0.923962  \n",
       "19          0.995153  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load resources\n",
    "tokenizer_hate = AutoTokenizer.from_pretrained(\"./Semi_SupervisedTrainingHateBERT\")\n",
    "model_hate = AutoModelForSequenceClassification.from_pretrained(\"./Semi_SupervisedTrainingHateBERT\")\n",
    "\n",
    "# Preprocess comments and get the preprocessed text\n",
    "comments_data = preprocess_comments(mock_comments_df)\n",
    "preprocessed_texts = comments_data['preprocessed_text'].to_list()\n",
    "\n",
    "# Tokenize preprocessed texts using the hate speech tokenizer\n",
    "encodings_hate = tokenizer_hate(preprocessed_texts, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "# Create dataset for hate speech predictions\n",
    "dataset_hate = Dataset.from_dict({\n",
    "    'input_ids': encodings_hate['input_ids'],\n",
    "    'attention_mask': encodings_hate['attention_mask']\n",
    "})\n",
    "\n",
    "# Make predictions\n",
    "predictions_df = predict_hate(dataset_hate, model_hate, tokenizer_hate)\n",
    "\n",
    "# You can then save the DataFrame to a CSV file if needed\n",
    "predictions_file = \"finalHatePredictions.csv\"\n",
    "predictions_df.to_csv(predictions_file, index=False)\n",
    "predictions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad68110",
   "metadata": {},
   "source": [
    "# Counter Speech Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d16e643",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3017289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to prepare data\n",
    "def prepare_data(subdialogues_df):\n",
    "    pairs = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(0, len(subdialogues_df) - 1, 2):\n",
    "        text1 = preprocess_text(subdialogues_df.iloc[i]['text'])\n",
    "        text2 = preprocess_text(subdialogues_df.iloc[i + 1]['text'])\n",
    "        combined_text = \"[CLS] \" + text1 + \" [SEP] \" + text2\n",
    "        combined_label = [subdialogues_df.iloc[i]['label'], subdialogues_df.iloc[i + 1]['label']]\n",
    "        pairs.append(combined_text)\n",
    "        labels.append(combined_label)\n",
    "\n",
    "    return pd.DataFrame({'text': pairs, 'label': labels})\n",
    "\n",
    "# Define the function to prepare data\n",
    "def prepare_data_TikTok(subdialogues_df):\n",
    "    pairs = []\n",
    "    \n",
    "    for i in range(0, len(subdialogues_df) - 1, 2):\n",
    "        text1 = preprocess_text(subdialogues_df.iloc[i]['text'])\n",
    "        text2 = preprocess_text(subdialogues_df.iloc[i + 1]['text'])\n",
    "        combined_text = \"[CLS] \" + text1 + \" [SEP] \" + text2\n",
    "        pairs.append(combined_text)\n",
    "\n",
    "    return pd.DataFrame({'text': pairs})\n",
    "\n",
    "\n",
    "# Define the function to prepare data\n",
    "def prepare_data_TikTok(df):\n",
    "    pairs = []\n",
    "    grouped = df.groupby('dialogue_id')\n",
    "    for _, group in grouped:\n",
    "        if len(group) % 2 != 0:\n",
    "            group = group[:-1]\n",
    "        \n",
    "        for i in range(0, len(group), 2):\n",
    "            combined_text = \"[CLS] \" + preprocess_text(group.iloc[i]['text']) + \" [SEP] \" + preprocess_text(group.iloc[i+1]['text'])\n",
    "            pairs.append(combined_text)\n",
    "    return pd.DataFrame({'text': pairs})\n",
    "\n",
    "\n",
    "def predict(dataloader, model):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            probs = torch.nn.functional.softmax(logits, dim=1).cpu().numpy()\n",
    "            \n",
    "            all_preds.extend(probs)\n",
    "    return all_preds\n",
    "\n",
    "\n",
    "def assign_labels(df, threshold=0.8, quantile=None, is_counter_speech=True):\n",
    "    labels = []\n",
    "    for _, row in df.iterrows():\n",
    "        prob = row[['probability_1', 'probability_2']].values\n",
    "        max_prob = max(prob)\n",
    "        index_max_prob = prob.argmax()\n",
    "        \n",
    "        if prob[1] > threshold:\n",
    "            label = [0,1]\n",
    "        elif prob[0] > threshold:\n",
    "            label = [1,0]\n",
    "        else:\n",
    "            label = [0,0]\n",
    "        \n",
    "        labels.append(label)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "afaa3d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class DialogueDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts.iloc[idx]\n",
    "        label = self.labels.iloc[idx]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=False,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label.values, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "class PredictionDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=128):\n",
    "        self.texts = df['text'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten()\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0427264b",
   "metadata": {},
   "source": [
    "## Training Model with labeled data from literature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c8055d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dialogue_id</th>\n",
       "      <th>text</th>\n",
       "      <th>turn_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>You're so stupid, I can't believe anyone liste...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "      <td>It's important to stay respectful even when we...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1002</td>\n",
       "      <td>Why do people like you always spread misinform...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1002</td>\n",
       "      <td>Everyone has different opinions, and that's okay.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1003</td>\n",
       "      <td>People like you are ruining the discussion wit...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dialogue_id                                               text  turn_id  \\\n",
       "0        1001  You're so stupid, I can't believe anyone liste...        0   \n",
       "1        1001  It's important to stay respectful even when we...        1   \n",
       "2        1002  Why do people like you always spread misinform...        0   \n",
       "3        1002  Everyone has different opinions, and that's okay.        1   \n",
       "4        1003  People like you are ruining the discussion wit...        0   \n",
       "\n",
       "   label  \n",
       "0      0  \n",
       "1      1  \n",
       "2      0  \n",
       "3      1  \n",
       "4      0  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    'dialogue_id': [\n",
    "        '1001', '1001', '1002', '1002', '1003',\n",
    "        '1003', '1004', '1004', '1005', '1005',\n",
    "        '1006', '1006', '1007', '1007', '1008',\n",
    "        '1008', '1009', '1009', '1010', '1010',\n",
    "        '1011', '1011', '1012', '1012', '1013',\n",
    "        '1013', '1014', '1014', '1015', '1015',\n",
    "        '1016', '1016', '1017', '1017', '1018',\n",
    "        '1018', '1019', '1019', '1020', '1020'\n",
    "    ],\n",
    "    'text': [\n",
    "        \"You're so stupid, I can't believe anyone listens to you.\",  # HS\n",
    "        \"It's important to stay respectful even when we disagree.\",  # CN\n",
    "        \"Why do people like you always spread misinformation?\",  # HS\n",
    "        \"Everyone has different opinions, and that's okay.\",  # CN\n",
    "        \"People like you are ruining the discussion with your hatred.\",  # HS\n",
    "        \"Let's focus on constructive dialogue rather than insults.\",  # CN\n",
    "        \"I find your comments deeply offensive and ignorant.\",  # HS\n",
    "        \"We should be more empathetic and try to understand each other's perspectives.\",  # CN\n",
    "        \"This kind of hate speech should not be tolerated.\",  # HS\n",
    "        \"Respectful conversation is key to resolving conflicts.\",  # CN\n",
    "        \"Your views are so outdated and discriminatory.\",  # HS\n",
    "        \"We can have a meaningful debate without resorting to personal attacks.\",  # CN\n",
    "        \"You only spread negativity and hate.\",  # HS\n",
    "        \"Constructive criticism is a way to improve, not to insult.\",  # CN\n",
    "        \"This rhetoric only fosters division and hatred.\",  # HS\n",
    "        \"Let's engage in discussions that promote mutual respect.\",  # CN\n",
    "        \"Hate speech has no place in our society.\",  # HS\n",
    "        \"Understanding and compassion can bridge gaps between us.\",  # CN\n",
    "        \"Your comment is a perfect example of toxic behavior.\",  # HS\n",
    "        \"We should strive for a more inclusive and respectful dialogue.\",  # CN\n",
    "        \"I can't believe how ignorant some people can be.\",  # HS\n",
    "        \"Engaging in respectful dialogue can help us learn from each other.\",  # CN\n",
    "        \"Your argument is completely unfounded and offensive.\",  # HS\n",
    "        \"Let's focus on finding common ground rather than attacking each other.\",  # CN\n",
    "        \"Your language is very hurtful and damaging.\",  # HS\n",
    "        \"Promoting understanding is essential for constructive discussions.\",  # CN\n",
    "        \"This kind of dialogue only escalates conflict.\",  # HS\n",
    "        \"We should aim for conversations that are both respectful and productive.\",  # CN\n",
    "        \"Your statements are harmful and unnecessary.\",  # HS\n",
    "        \"Open-mindedness and respect are crucial for meaningful exchanges.\",  # CN\n",
    "        \"This kind of negativity is detrimental to productive conversation.\",  # HS\n",
    "        \"Constructive dialogue involves listening and understanding.\",  # CN\n",
    "        \"Your comments only serve to create further division.\",  # HS\n",
    "        \"Let's work towards resolving our differences with mutual respect.\",  # CN\n",
    "        \"This type of rhetoric is unacceptable in any discussion.\",  # HS\n",
    "        \"We should always strive to communicate with empathy and respect.\",  # CN\n",
    "        \"Your behavior is completely unacceptable and damaging.\",  # HS\n",
    "        \"Respectful discourse is the foundation of any healthy discussion.\",  # CN\n",
    "        \"I do not like you.\", # HS\n",
    "        \"You should not answer like this.\" # CN\n",
    "    ],\n",
    "    'turn_id': [\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1\n",
    "    ],\n",
    "    'label': [\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1\n",
    "    ]\n",
    "}\n",
    "\n",
    "mock_dialogues_df = pd.DataFrame(data)\n",
    "mock_dialogues_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "118ef181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[CLS] youre so stupid i cant believe anyone li...</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[CLS] why do people like you always spread mis...</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[CLS] people like you are ruining the discussi...</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[CLS] i find your comments deeply offensive an...</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[CLS] this kind of hate speech should not be t...</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   label\n",
       "0  [CLS] youre so stupid i cant believe anyone li...  [0, 1]\n",
       "1  [CLS] why do people like you always spread mis...  [0, 1]\n",
       "2  [CLS] people like you are ruining the discussi...  [0, 1]\n",
       "3  [CLS] i find your comments deeply offensive an...  [0, 1]\n",
       "4  [CLS] this kind of hate speech should not be t...  [0, 1]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_df = prepare_data(mock_dialogues_df)\n",
    "pairs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "82763829",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.739102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.736809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.732828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset results: {'eval_loss': 0.7075212001800537, 'eval_runtime': 0.2827, 'eval_samples_per_second': 14.15, 'eval_steps_per_second': 3.538, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X = pairs_df['text']\n",
    "y = pd.DataFrame(pairs_df['label'].tolist(), columns=['label_1', 'label_2'])\n",
    "\n",
    "# Split into train (60%), val (20%), test (20%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Tokenizer and datasets\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_dataset = DialogueDataset(X_train, y_train, tokenizer, max_len=128)\n",
    "val_dataset = DialogueDataset(X_val, y_val, tokenizer, max_len=128)\n",
    "test_dataset = DialogueDataset(X_test, y_test, tokenizer, max_len=128)\n",
    "\n",
    "# Load model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "# Save model\n",
    "os.makedirs(\"./SupervisedTrainingDialogueBERT\", exist_ok=True)\n",
    "model.save_pretrained(\"./SupervisedTrainingDialogueBERT\")\n",
    "tokenizer.save_pretrained(\"./SupervisedTrainingDialogueBERT\")\n",
    "\n",
    "# Evaluate on the test dataset\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "print(\"Test dataset results:\", test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c479c9",
   "metadata": {},
   "source": [
    "## Predicting TikTok Dialogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7d706803",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = {\n",
    "    'dialogue_id': [\n",
    "        '3001', '3001', '3002', '3002', '3003',\n",
    "        '3003', '3004', '3004', '3005', '3005',\n",
    "        '3006', '3006', '3007', '3007', '3008',\n",
    "        '3008', '3009', '3009', '3010', '3010',\n",
    "        '3011', '3011', '3012', '3012', '3013',\n",
    "        '3013', '3014', '3014', '3015', '3015',\n",
    "        '3016', '3016', '3017', '3017', '3018',\n",
    "        '3018', '3019', '3019', '3020', '3020'\n",
    "    ],\n",
    "    'text': [\n",
    "        \"People like you are ruining this community with your hate.\",  # HS\n",
    "        \"Everyone deserves respect, regardless of their opinions.\",  # CN\n",
    "        \"Your arguments are so backward and offensive.\",  # HS\n",
    "        \"It's important to engage in discussions with empathy and understanding.\",  # CN\n",
    "        \"Why do people like you always spread negativity?\",  # HS\n",
    "        \"Constructive dialogue can bridge gaps and resolve conflicts.\",  # CN\n",
    "        \"Your comment is harmful and discriminatory.\",  # HS\n",
    "        \"Let's focus on positive and respectful communication.\",  # CN\n",
    "        \"You should be ashamed of your prejudiced views.\",  # HS\n",
    "        \"Everyone has different perspectives and that's okay.\",  # CN\n",
    "        \"This kind of rhetoric only perpetuates hatred.\",  # HS\n",
    "        \"Promoting mutual respect and understanding is crucial.\",  # CN\n",
    "        \"Your behavior is unacceptable and damaging.\",  # HS\n",
    "        \"We should strive for more inclusive and respectful conversations.\",  # CN\n",
    "        \"Hate speech has no place in a healthy dialogue.\",  # HS\n",
    "        \"Constructive criticism helps us grow, not tear us down.\",  # CN\n",
    "        \"This kind of negativity only fosters division.\",  # HS\n",
    "        \"Let's engage in discussions that promote empathy and learning.\",  # CN\n",
    "        \"Your language is hurtful and unnecessary.\",  # HS\n",
    "        \"Respectful dialogue is essential for productive conversations.\",  # CN\n",
    "        \"People like you are spreading false information.\",  # HS\n",
    "        \"We can have disagreements without resorting to personal attacks.\",  # CN\n",
    "        \"Your comments are divisive and harmful.\",  # HS\n",
    "        \"Understanding different viewpoints can lead to meaningful discussions.\",  # CN\n",
    "        \"You only contribute to the problem with your hateful speech.\",  # HS\n",
    "        \"Empathy and respect are the foundation of any healthy debate.\",  # CN\n",
    "        \"This kind of discourse is unacceptable and damaging.\",  # HS\n",
    "        \"Promoting kindness and open-mindedness can make a difference.\",  # CN\n",
    "        \"Your views are toxic and unproductive.\",  # HS\n",
    "        \"Let's work towards creating a more respectful and understanding environment.\",  # CN\n",
    "        \"Your comments are inflammatory and disrespectful.\",  # HS\n",
    "        \"Focusing on common ground can help us resolve conflicts.\",  # CN\n",
    "        \"Your attitude is completely unacceptable.\",  # HS\n",
    "        \"We should always strive for constructive and respectful conversations.\",  # CN\n",
    "        \"Your statements are prejudiced and harmful.\",  # HS\n",
    "        \"Encouraging respectful dialogue can lead to positive change.\",  # CN\n",
    "        \"Your argument is completely unfounded and offensive.\",  # HS\n",
    "        \"Let's focus on finding common ground rather than attacking each other.\",  # CN\n",
    "        \"Your language is very hurtful and damaging.\",  # HS\n",
    "        \"Promoting understanding is essential for constructive discussions.\",  # CN\n",
    "    ],\n",
    "    'turn_id': [\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1\n",
    "    ]\n",
    "}\n",
    "\n",
    "mock_dialogues_TikTok_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "48872b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities:\n",
      " [array([0.46400982, 0.5359902 ], dtype=float32), array([0.45302135, 0.5469786 ], dtype=float32), array([0.50846034, 0.49153963], dtype=float32), array([0.5130343 , 0.48696575], dtype=float32), array([0.502367  , 0.49763295], dtype=float32), array([0.4674845, 0.5325154], dtype=float32), array([0.50496083, 0.4950392 ], dtype=float32), array([0.47799498, 0.522005  ], dtype=float32), array([0.4412687 , 0.55873126], dtype=float32), array([0.48659128, 0.5134087 ], dtype=float32), array([0.43824032, 0.5617597 ], dtype=float32), array([0.5100657 , 0.48993438], dtype=float32), array([0.462044, 0.537956], dtype=float32), array([0.47992748, 0.52007246], dtype=float32), array([0.51272655, 0.48727348], dtype=float32), array([0.4572117 , 0.54278827], dtype=float32), array([0.4852293 , 0.51477075], dtype=float32), array([0.50630677, 0.4936932 ], dtype=float32), array([0.48597038, 0.5140296 ], dtype=float32), array([0.47975436, 0.5202456 ], dtype=float32)]\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./SupervisedTrainingDialogueBERT\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"./SupervisedTrainingDialogueBERT\")\n",
    "\n",
    "pairs_df_TikTok = prepare_data_TikTok(mock_dialogues_TikTok_df)\n",
    "# Create the dataset\n",
    "prediction_dataset = PredictionDataset(pairs_df_TikTok, tokenizer)\n",
    "\n",
    "# Create a dataloader\n",
    "dataloader = DataLoader(prediction_dataset, batch_size=8)\n",
    "\n",
    "# Get predictions\n",
    "probs = predict(dataloader, model)\n",
    "\n",
    "pairs_df_TikTok[\"probabilities\"] = probs\n",
    "print(\"Probabilities:\\n\", probs)\n",
    "print(len(probs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17221d57",
   "metadata": {},
   "source": [
    "## Obtain pseudolabels and retraining CS prediction model with labels and pseudo-labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "53f86450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text   label\n",
      "10  [CLS] people like you are spreading false info...  [0, 0]\n",
      "3   [CLS] your comment is harmful and discriminato...  [0, 0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[CLS] youre so stupid i cant believe anyone li...</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[CLS] why do people like you always spread mis...</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[CLS] people like you are ruining the discussi...</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[CLS] i find your comments deeply offensive an...</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[CLS] this kind of hate speech should not be t...</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[CLS] your views are so outdated and discrimin...</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[CLS] you only spread negativity and hate [SEP...</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[CLS] this rhetoric only fosters division and ...</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[CLS] hate speech has no place in our society ...</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[CLS] your comment is a perfect example of tox...</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[CLS] i cant believe how ignorant some people ...</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[CLS] your argument is completely unfounded an...</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[CLS] your language is very hurtful and damagi...</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[CLS] this kind of dialogue only escalates con...</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[CLS] your statements are harmful and unnecess...</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[CLS] this kind of negativity is detrimental t...</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[CLS] your comments only serve to create furth...</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[CLS] this type of rhetoric is unacceptable in...</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[CLS] your behavior is completely unacceptable...</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[CLS] i do not like you [SEP] you should not a...</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[CLS] people like you are spreading false info...</td>\n",
       "      <td>[0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[CLS] your comment is harmful and discriminato...</td>\n",
       "      <td>[0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text   label\n",
       "0   [CLS] youre so stupid i cant believe anyone li...  [0, 1]\n",
       "1   [CLS] why do people like you always spread mis...  [0, 1]\n",
       "2   [CLS] people like you are ruining the discussi...  [0, 1]\n",
       "3   [CLS] i find your comments deeply offensive an...  [0, 1]\n",
       "4   [CLS] this kind of hate speech should not be t...  [0, 1]\n",
       "5   [CLS] your views are so outdated and discrimin...  [0, 1]\n",
       "6   [CLS] you only spread negativity and hate [SEP...  [0, 1]\n",
       "7   [CLS] this rhetoric only fosters division and ...  [0, 1]\n",
       "8   [CLS] hate speech has no place in our society ...  [0, 1]\n",
       "9   [CLS] your comment is a perfect example of tox...  [0, 1]\n",
       "10  [CLS] i cant believe how ignorant some people ...  [0, 1]\n",
       "11  [CLS] your argument is completely unfounded an...  [0, 1]\n",
       "12  [CLS] your language is very hurtful and damagi...  [0, 1]\n",
       "13  [CLS] this kind of dialogue only escalates con...  [0, 1]\n",
       "14  [CLS] your statements are harmful and unnecess...  [0, 1]\n",
       "15  [CLS] this kind of negativity is detrimental t...  [0, 1]\n",
       "16  [CLS] your comments only serve to create furth...  [0, 1]\n",
       "17  [CLS] this type of rhetoric is unacceptable in...  [0, 1]\n",
       "18  [CLS] your behavior is completely unacceptable...  [0, 1]\n",
       "19  [CLS] i do not like you [SEP] you should not a...  [0, 1]\n",
       "20  [CLS] people like you are spreading false info...  [0, 0]\n",
       "21  [CLS] your comment is harmful and discriminato...  [0, 0]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_df_TikTok[['probability_1', 'probability_2']] = pd.DataFrame(pairs_df_TikTok['probabilities'].tolist(), index=pairs_df_TikTok.index)\n",
    "\n",
    "# Variables to control the pseudolabeling process\n",
    "counter_speech_probability_threshold_pseudolabel = False\n",
    "non_counter_speech_probability_threshold_pseudolabel = False\n",
    "\n",
    "# --- Pseudolabeling for high-probability counter speech ---\n",
    "if counter_speech_probability_threshold_pseudolabel:\n",
    "    probability_threshold = 0.95\n",
    "    top_df = pairs_df_TikTok[pairs_df_TikTok['probability_2'] > probability_threshold].copy()\n",
    "    top_df['label'] = assign_labels(top_df, threshold=probability_threshold, is_counter_speech=True)\n",
    "else:\n",
    "    quantile = 0.05\n",
    "    top_cutoff = pairs_df_TikTok['probability_2'].quantile(1 - quantile)\n",
    "    top_df = pairs_df_TikTok[pairs_df_TikTok['probability_2'] >= top_cutoff].copy()\n",
    "    top_df['label'] = assign_labels(top_df, quantile=quantile, is_counter_speech=True)\n",
    "\n",
    "# --- Pseudolabeling for low-probability (non-counter) speech ---\n",
    "if non_counter_speech_probability_threshold_pseudolabel:\n",
    "    probability_threshold = 0.05\n",
    "    bottom_df = pairs_df_TikTok[pairs_df_TikTok['probability_2'] < probability_threshold].copy()\n",
    "    bottom_df['label'] = assign_labels(bottom_df, threshold=probability_threshold, is_counter_speech=False)\n",
    "else:\n",
    "    quantile = 0.05\n",
    "    bottom_cutoff = pairs_df_TikTok['probability_2'].quantile(quantile)\n",
    "    bottom_df = pairs_df_TikTok[pairs_df_TikTok['probability_2'] <= bottom_cutoff].copy()\n",
    "    bottom_df['label'] = assign_labels(bottom_df, quantile=quantile, is_counter_speech=False)\n",
    "\n",
    "# --- Combine top and bottom pseudolabels ---\n",
    "counter_speech_pseudolabels = pd.concat([top_df, bottom_df])[[\"text\", \"label\"]]\n",
    "print(counter_speech_pseudolabels)\n",
    "\n",
    "# --- Combine with existing labeled data ---\n",
    "pairs_df_labeled = prepare_data(mock_dialogues_df)\n",
    "pairs_df = pd.concat([pairs_df_labeled, counter_speech_pseudolabels]).reset_index(drop=True)\n",
    "pairs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "89ad0004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:11, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.728548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.727573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.725780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6, training_loss=0.6690843105316162, metrics={'train_runtime': 12.9471, 'train_samples_per_second': 2.781, 'train_steps_per_second': 0.463, 'total_flos': 2367999498240.0, 'train_loss': 0.6690843105316162, 'epoch': 3.0})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pairs_df['text']\n",
    "y = pd.DataFrame(pairs_df['label'].tolist(), columns=['label_1', 'label_2'])\n",
    "\n",
    "\n",
    "# Split into train (60%), val (20%), test (20%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)\n",
    "\n",
    "# Tokenizer and datasets\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_dataset = DialogueDataset(X_train, y_train, tokenizer, max_len=128)\n",
    "val_dataset = DialogueDataset(X_validation, y_validation, tokenizer, max_len=128)\n",
    "test_dataset = DialogueDataset(X_test, y_test, tokenizer, max_len=128)\n",
    "\n",
    "\n",
    "# Load model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    eval_strategy='epoch',  # Evaluate on val set each epoch\n",
    "    save_strategy='epoch',\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3229dbcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset results: {'eval_loss': 0.6908215284347534, 'eval_runtime': 0.327, 'eval_samples_per_second': 15.29, 'eval_steps_per_second': 3.058, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"./SemiSupervisedTrainingDialogueBERT\", exist_ok=True)\n",
    "model.save_pretrained(\"./SemiSupervisedTrainingDialogueBERT\")\n",
    "tokenizer.save_pretrained(\"./SemiSupervisedTrainingDialogueBERT\")\n",
    "\n",
    "# Evaluate on the test dataset\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "print(\"Test dataset results:\", test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f79b68",
   "metadata": {},
   "source": [
    "## Predicting final CS probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "458d5fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities:\n",
      " [array([0.46406472, 0.5359353 ], dtype=float32), array([0.4526586, 0.5473414], dtype=float32), array([0.5084366 , 0.49156335], dtype=float32), array([0.5125879 , 0.48741212], dtype=float32), array([0.5024599 , 0.49754018], dtype=float32), array([0.46737164, 0.53262836], dtype=float32), array([0.5049363 , 0.49506372], dtype=float32), array([0.4778811, 0.5221189], dtype=float32), array([0.44102094, 0.55897903], dtype=float32), array([0.48624855, 0.51375145], dtype=float32), array([0.43817946, 0.56182057], dtype=float32), array([0.5095751 , 0.49042487], dtype=float32), array([0.46177673, 0.53822327], dtype=float32), array([0.47973704, 0.52026296], dtype=float32), array([0.51225865, 0.48774135], dtype=float32), array([0.45677826, 0.5432217 ], dtype=float32), array([0.48526284, 0.5147372 ], dtype=float32), array([0.50586224, 0.49413773], dtype=float32), array([0.48558873, 0.5144112 ], dtype=float32), array([0.47942236, 0.52057767], dtype=float32)]\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./SemiSupervisedTrainingDialogueBERT\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"./SemiSupervisedTrainingDialogueBERT\")\n",
    "\n",
    "pairs_df_TikTok = prepare_data_TikTok(mock_dialogues_TikTok_df)\n",
    "# Create the dataset\n",
    "prediction_dataset = PredictionDataset(pairs_df_TikTok, tokenizer)\n",
    "\n",
    "# Create a dataloader\n",
    "dataloader = DataLoader(prediction_dataset, batch_size=8)\n",
    "\n",
    "# Get predictions\n",
    "probs = predict(dataloader, model)\n",
    "\n",
    "pairs_df_TikTok[\"probabilities\"] = probs\n",
    "print(\"Probabilities:\\n\", probs)\n",
    "print(len(probs))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
